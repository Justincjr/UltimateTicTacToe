{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split first then augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from utils import State, Action, load_data  # Ensure your utils.py is in your directory\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data size: 80000\n",
      "Train data size: 68000 | Test data size: 12000\n",
      "Final augmented train data size: 1085528\n",
      "Final augmented test data size: 191896\n",
      "Augmented and feature-extracted datasets have been saved.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from utils import State, load_data\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split  # Ensure scikit-learn is installed\n",
    "\n",
    "# (Your existing functions like convert_board_to_string, get_local_board_status, etc. are assumed to be defined/imported)\n",
    "\n",
    "def update_prev_local_action(prev: tuple, transform: str) -> tuple:\n",
    "    r, c = prev\n",
    "    if transform == \"identity\":\n",
    "        return (r, c)\n",
    "    elif transform == \"horizontal\":\n",
    "        return (r, 2 - c)\n",
    "    elif transform == \"vertical\":\n",
    "        return (2 - r, c)\n",
    "    elif transform == \"rotate90\":\n",
    "        return (c, 2 - r)\n",
    "    elif transform == \"rotate180\":\n",
    "        return (2 - r, 2 - c)\n",
    "    elif transform == \"rotate270\":\n",
    "        return (2 - c, r)\n",
    "    elif \"_\" in transform:\n",
    "        base, extra = transform.split(\"_\")\n",
    "        if base == \"rotate90\":\n",
    "            new = (c, 2 - r)\n",
    "        elif base == \"rotate180\":\n",
    "            new = (2 - r, 2 - c)\n",
    "        elif base == \"rotate270\":\n",
    "            new = (2 - c, r)\n",
    "        else:\n",
    "            new = (r, c)\n",
    "        if extra == \"horizontal\":\n",
    "            new = (new[0], 2 - new[1])\n",
    "        elif extra == \"vertical\":\n",
    "            new = (2 - new[0], new[1])\n",
    "        return new\n",
    "    else:\n",
    "        return (r, c)\n",
    "\n",
    "def transform_state(state: State, transform: str) -> State:\n",
    "    global_board = state.board.transpose(0, 2, 1, 3).reshape(9, 9)\n",
    "\n",
    "    if transform == \"identity\":\n",
    "        transformed_global_board = global_board.copy()\n",
    "    elif transform == \"horizontal\":\n",
    "        transformed_global_board = np.fliplr(global_board)\n",
    "    elif transform == \"vertical\":\n",
    "        transformed_global_board = np.flipud(global_board)\n",
    "    elif transform.startswith(\"rotate\"):\n",
    "        if \"_\" in transform:\n",
    "            base, extra = transform.split(\"_\")\n",
    "            if base == \"rotate90\":\n",
    "                k = 1\n",
    "            elif base == \"rotate180\":\n",
    "                k = 2\n",
    "            elif base == \"rotate270\":\n",
    "                k = 3\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown base rotation: {base}\")\n",
    "            transformed_global_board = np.rot90(global_board, k=k)\n",
    "            if extra == \"horizontal\":\n",
    "                transformed_global_board = np.fliplr(transformed_global_board)\n",
    "            elif extra == \"vertical\":\n",
    "                transformed_global_board = np.flipud(transformed_global_board)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown extra transform: {extra}\")\n",
    "        else:\n",
    "            if transform == \"rotate90\":\n",
    "                k = 1\n",
    "            elif transform == \"rotate180\":\n",
    "                k = 2\n",
    "            elif transform == \"rotate270\":\n",
    "                k = 3\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown rotation transform: {transform}\")\n",
    "            transformed_global_board = np.rot90(global_board, k=k)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown transform: {transform}\")\n",
    "\n",
    "    new_board = transformed_global_board.reshape(3, 3, 3, 3).transpose(0, 2, 1, 3)\n",
    "    new_prev = None\n",
    "    if state.prev_local_action is not None:\n",
    "        new_prev = update_prev_local_action(state.prev_local_action, transform)\n",
    "\n",
    "    return State(board=new_board, fill_num=state.fill_num, prev_local_action=new_prev)\n",
    "\n",
    "def augment_entire_state(state: State) -> list[State]:\n",
    "    transforms = [\n",
    "        \"identity\", \"horizontal\", \"vertical\",\n",
    "        \"rotate90\", \"rotate90_horizontal\", \"rotate90_vertical\",\n",
    "        \"rotate180\", \"rotate180_horizontal\", \"rotate180_vertical\",\n",
    "        \"rotate270\", \"rotate270_horizontal\", \"rotate270_vertical\"\n",
    "    ]\n",
    "    return [transform_state(state, t) for t in transforms]\n",
    "\n",
    "def state_to_key(state: State) -> tuple:\n",
    "    return (state.board.tobytes(), state.fill_num, state.prev_local_action)\n",
    "\n",
    "def augment_dataset(data: list[tuple[State, float]]) -> list[tuple[State, float]]:\n",
    "    augmented_data = []\n",
    "    for state, utility in data:\n",
    "        for aug_state in augment_entire_state(state):\n",
    "            augmented_data.append((aug_state, utility))\n",
    "    unique_data = {}\n",
    "    for state, utility in augmented_data:\n",
    "        key = state_to_key(state)\n",
    "        if key not in unique_data:\n",
    "            unique_data[key] = (state, utility)\n",
    "    return list(unique_data.values())\n",
    "\n",
    "def swap_state(state: State) -> State:\n",
    "    new_board = np.where(state.board == 1, 2, np.where(state.board == 2, 1, state.board))\n",
    "    new_fill_num = 3 - state.fill_num\n",
    "    return State(board=new_board, fill_num=new_fill_num, prev_local_action=state.prev_local_action)\n",
    "\n",
    "def augment_dataset_with_swap(data: list[tuple[State, float]]) -> list[tuple[State, float]]:\n",
    "    swapped_data = []\n",
    "    for state, utility in data:\n",
    "        swapped_state = swap_state(state)\n",
    "        swapped_utility = -utility\n",
    "        swapped_data.append((swapped_state, swapped_utility))\n",
    "    \n",
    "    unique_data = {}\n",
    "    for state, utility in swapped_data:\n",
    "        key = state_to_key(state)\n",
    "        if key not in unique_data:\n",
    "            unique_data[key] = (state, utility)\n",
    "    return list(unique_data.values())\n",
    "\n",
    "# --- Custom Encoding Functions ---\n",
    "\n",
    "def custom_encode_2(val: int) -> np.ndarray:\n",
    "    if val == 0:\n",
    "        return np.array([0, 0], dtype=np.float32)\n",
    "    elif val == 1:\n",
    "        return np.array([0, 1], dtype=np.float32)\n",
    "    elif val == 2:\n",
    "        return np.array([1, 0], dtype=np.float32)\n",
    "    else:\n",
    "        raise ValueError(\"Value must be 0, 1, or 2.\")\n",
    "\n",
    "def custom_encode_status(val: int) -> np.ndarray:\n",
    "    if val == 0:\n",
    "        return np.array([0, 0, 0], dtype=np.float32)\n",
    "    elif val == 1:\n",
    "        return np.array([0, 0, 1], dtype=np.float32)\n",
    "    elif val == 2:\n",
    "        return np.array([0, 1, 0], dtype=np.float32)\n",
    "    elif val == 3:\n",
    "        return np.array([1, 0, 0], dtype=np.float32)\n",
    "    else:\n",
    "        raise ValueError(\"Local board status must be 0, 1, 2, or 3.\")\n",
    "\n",
    "def custom_encode_coord(val: int) -> np.ndarray:\n",
    "    if val < 0:\n",
    "        return np.array([0, 0], dtype=np.float32)\n",
    "    return custom_encode_2(val)\n",
    "\n",
    "# --- Main processing: splitting, augmenting, feature extraction ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the original dataset (each element is a (State, utility) pair).\n",
    "    original_data = load_data()\n",
    "    print(f\"Original data size: {len(original_data)}\")\n",
    "\n",
    "    # --- Split the dataset into train and test sets (80/20 split) ---\n",
    "    train_data, test_data = train_test_split(original_data, test_size=0.15, random_state=42)\n",
    "    print(f\"Train data size: {len(train_data)} | Test data size: {len(test_data)}\")\n",
    "\n",
    "    # --- Augment the training set ---\n",
    "    sym_aug_train = augment_dataset(train_data)\n",
    "    swap_aug_train = augment_dataset_with_swap(sym_aug_train)\n",
    "    combined_train = sym_aug_train + swap_aug_train\n",
    "    unique_train = {}\n",
    "    for state, utility in combined_train:\n",
    "        key = state_to_key(state)\n",
    "        if key not in unique_train:\n",
    "            unique_train[key] = (state, utility)\n",
    "    final_train = list(unique_train.values())\n",
    "    print(f\"Final augmented train data size: {len(final_train)}\")\n",
    "\n",
    "    # --- Augment the test set ---\n",
    "    sym_aug_test = augment_dataset(test_data)\n",
    "    swap_aug_test = augment_dataset_with_swap(sym_aug_test)\n",
    "    combined_test = sym_aug_test + swap_aug_test\n",
    "    unique_test = {}\n",
    "    for state, utility in combined_test:\n",
    "        key = state_to_key(state)\n",
    "        if key not in unique_test:\n",
    "            unique_test[key] = (state, utility)\n",
    "    final_test = list(unique_test.values())\n",
    "    print(f\"Final augmented test data size: {len(final_test)}\")\n",
    "\n",
    "    # --- Feature Extraction ---\n",
    "    # Define a helper function for feature extraction.\n",
    "    def extract_features(state: State) -> np.ndarray:\n",
    "        # Global board: shape (3,3,3,3) -> flatten to 81 elements; encode each cell into 2 features.\n",
    "        board_flat = state.board.reshape(-1)  # 81 elements\n",
    "        global_board_encoded = np.array([custom_encode_2(val) for val in board_flat])\n",
    "        global_board_features = global_board_encoded.flatten()  # 81*2 = 162 features\n",
    "\n",
    "        # Local board status: assume state.local_board_status exists with shape (3,3) -> 9 elements; encode each into 3 features.\n",
    "        local_board_flat = state.local_board_status.reshape(-1)\n",
    "        local_board_encoded = np.array([custom_encode_status(val) for val in local_board_flat])\n",
    "        local_board_features = local_board_encoded.flatten()  # 9*3 = 27 features\n",
    "\n",
    "        # Fill number: encode into 2 features.\n",
    "        fill_num_feature = custom_encode_2(state.fill_num)  # 2 features\n",
    "\n",
    "        # Previous local action: encode each coordinate into 2 features.\n",
    "        if state.prev_local_action is None:\n",
    "            prev_r, prev_c = -1, -1\n",
    "        else:\n",
    "            prev_r, prev_c = state.prev_local_action\n",
    "        prev_r_enc = custom_encode_coord(prev_r)  # 2 features\n",
    "        prev_c_enc = custom_encode_coord(prev_c)  # 2 features\n",
    "        prev_action = np.concatenate([prev_r_enc, prev_c_enc])  # 4 features\n",
    "\n",
    "        # Total features: 162 + 27 + 2 + 4 = 195\n",
    "        features = np.concatenate([global_board_features, local_board_features, fill_num_feature, prev_action])\n",
    "        return features\n",
    "\n",
    "    # Process training set features.\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for state, utility in final_train:\n",
    "        features = extract_features(state)\n",
    "        X_train.append(features)\n",
    "        y_train.append(utility)\n",
    "\n",
    "    # Process test set features.\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for state, utility in final_test:\n",
    "        features = extract_features(state)\n",
    "        X_test.append(features)\n",
    "        y_test.append(utility)\n",
    "\n",
    "    # --- Save the final augmented datasets (features and labels) ---\n",
    "    data = {\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "\n",
    "    # Save to a single pickle file.\n",
    "    with open(\"features15.pkl\", \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "    print(\"Augmented and feature-extracted datasets have been saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"features15.pkl\", \"rb\") as f:\n",
    "    data_loaded = pickle.load(f)\n",
    "\n",
    "X_train = data_loaded['X_train']\n",
    "y_train = data_loaded['y_train']\n",
    "X_eval = data_loaded['X_test']\n",
    "y_eval = data_loaded['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 50/1200, Training Loss: 0.2821\n",
      "Epoch 100/1200, Training Loss: 0.2473\n",
      "Epoch 150/1200, Training Loss: 0.2179\n",
      "Epoch 200/1200, Training Loss: 0.1957\n",
      "Epoch 250/1200, Training Loss: 0.1823\n",
      "Epoch 300/1200, Training Loss: 0.1743\n",
      "Epoch 350/1200, Training Loss: 0.1681\n",
      "Epoch 400/1200, Training Loss: 0.1636\n",
      "Epoch 450/1200, Training Loss: 0.1594\n",
      "Epoch 500/1200, Training Loss: 0.1572\n",
      "Epoch 550/1200, Training Loss: 0.1529\n",
      "Epoch 600/1200, Training Loss: 0.1504\n",
      "Epoch 650/1200, Training Loss: 0.1480\n",
      "Epoch 700/1200, Training Loss: 0.1452\n",
      "Epoch 750/1200, Training Loss: 0.1428\n",
      "Epoch 800/1200, Training Loss: 0.1408\n",
      "Epoch 850/1200, Training Loss: 0.1391\n",
      "Epoch 900/1200, Training Loss: 0.1372\n",
      "Epoch 950/1200, Training Loss: 0.1358\n",
      "Epoch 1000/1200, Training Loss: 0.1341\n",
      "Epoch 1050/1200, Training Loss: 0.1335\n",
      "Epoch 1100/1200, Training Loss: 0.1320\n",
      "Epoch 1150/1200, Training Loss: 0.1312\n",
      "Epoch 1200/1200, Training Loss: 0.1308\n",
      "Epoch 1200, Training Loss: 0.1308\n",
      "Evaluation MSE: 0.1313\n"
     ]
    }
   ],
   "source": [
    "# one hot encoding\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pprint\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(195, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(0.45)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(0.20)\n",
    "\n",
    "        self.out = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "# Set up GPU device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_eval_tensor = torch.tensor(X_eval, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)  # Add extra dimension\n",
    "y_eval_tensor = torch.tensor(y_eval, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "# --- Training ---\n",
    "net = Net().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "losses = []\n",
    "\n",
    "num_epochs = 1200\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    y_pred_train = net(X_train_tensor)  # Use the tensor version.\n",
    "    loss = loss_fn(y_pred_train, y_train_tensor)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item():.4f}\")\n",
    "    if epoch + 1 == num_epochs:\n",
    "        print(f\"Epoch {num_epochs}, Training Loss: {loss.item():.4f}\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_eval = net(X_eval_tensor)\n",
    "eval_mse = loss_fn(y_pred_eval, y_eval_tensor).item()\n",
    "print(f\"Evaluation MSE: {eval_mse:.4f}\")\n",
    "\n",
    "trained_weights = net.state_dict()\n",
    "weights_dict = OrderedDict()\n",
    "for name, param in trained_weights.items():\n",
    "    values = param.detach().cpu().numpy().tolist()\n",
    "    weights_dict[name] = values\n",
    "\n",
    "with open(\"weights.py\", \"w\") as f:\n",
    "    f.write(\"weights = OrderedDict([\\n\")\n",
    "    for key, value in weights_dict.items():\n",
    "        f.write(f\"    ('{key}', torch.tensor(\\n\")\n",
    "        f.write(pprint.pformat(value, indent=8))\n",
    "        f.write(\"\\n    )),\\n\")  # close parentheses here\n",
    "    f.write(\"])\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS2109S",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
